{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cebd973-45a3-4e02-9ca2-7e23cb354f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "!pip install rdflib\n",
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c92325-7633-44b4-a6cf-f2398569b48a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Load first 100 rows from the silver layer of Yellow Taxi trips\n",
    "df = spark.table(\"silver_layer_yellow_taxi_including_features\").limit(100)\n",
    "\n",
    "# Import rdflib for creating RDF graphs\n",
    "import rdflib\n",
    "from rdflib import Namespace, Literal, RDF, XSD\n",
    "\n",
    "# Create an empty RDF graph\n",
    "g = rdflib.Graph()  \n",
    "\n",
    "# Define a namespace for all our taxi-related RDF entities\n",
    "TAXI = Namespace(\"http://bosch.org/nytaxi/\")\n",
    "g.bind(\"taxi\", TAXI)  # bind prefix for readability in serialized graphs\n",
    "\n",
    "# Helper function to create unique URIs for entities (Trips, Locations, etc.)\n",
    "def make_uri(entity_type, entity_id):\n",
    "    return TAXI[f\"{entity_type}/{entity_id}\"]\n",
    "\n",
    "# Transform each row in the DataFrame into RDF triples\n",
    "# This is the core of the RDF modeling: each row becomes a Trip entity with linked Locations\n",
    "for idx, row in enumerate(df.collect()):\n",
    "    trip_uri = make_uri(\"Trip\", idx+1)  # create a unique URI for this trip\n",
    "    pickup_loc_uri = make_uri(\"Location\", row[\"PULocationID\"])\n",
    "    dropoff_loc_uri = make_uri(\"Location\", row[\"DOLocationID\"])\n",
    "    \n",
    "    # Add RDF type statements\n",
    "    g.add((trip_uri, RDF.type, TAXI.Trip))\n",
    "    g.add((pickup_loc_uri, RDF.type, TAXI.Location))\n",
    "    g.add((dropoff_loc_uri, RDF.type, TAXI.Location))\n",
    "    \n",
    "    # Link the trip to its pickup and dropoff locations\n",
    "    g.add((trip_uri, TAXI.pickupLocation, pickup_loc_uri))\n",
    "    g.add((trip_uri, TAXI.dropoffLocation, dropoff_loc_uri))\n",
    "    \n",
    "    # Add literal properties for the trip (times, distance, fare, passengers)\n",
    "    g.add((trip_uri, TAXI.pickupTime, Literal(row[\"tpep_pickup_datetime\"], datatype=XSD.dateTime)))\n",
    "    g.add((trip_uri, TAXI.dropoffTime, Literal(row[\"tpep_dropoff_datetime\"], datatype=XSD.dateTime)))\n",
    "    g.add((trip_uri, TAXI.tripDistance, Literal(row[\"trip_distance\"], datatype=XSD.float)))\n",
    "    g.add((trip_uri, TAXI.fareAmount, Literal(row[\"fare_amount\"], datatype=XSD.decimal)))\n",
    "    g.add((trip_uri, TAXI.passengerCount, Literal(row[\"passenger_count\"], datatype=XSD.int)))\n",
    "\n",
    "# Convert RDF graph into a list of triples (subject, predicate, object)\n",
    "triples_list = [(str(s), str(p), str(o)) for s, p, o in g]\n",
    "\n",
    "# Define Spark schema for storing triples in a DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"s\", StringType(), True),\n",
    "    StructField(\"p\", StringType(), True),\n",
    "    StructField(\"o\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a Spark DataFrame from the RDF triples\n",
    "triples_df = spark.createDataFrame(triples_list, schema)\n",
    "\n",
    "# Save the triples as a Delta/Hive table for downstream use\n",
    "triples_df.write.mode(\"overwrite\").saveAsTable(\"gold_yellow_taxi_rdf_triples\")\n",
    "\n",
    "# Example SPARQL query: select trips within a certain pickup time range\n",
    "query_hardcoded = \"\"\"\n",
    "PREFIX taxi: <http://bosch.org/nytaxi/>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "SELECT ?trip ?pickupTime ?dropoffTime\n",
    "WHERE {\n",
    "    ?trip taxi:pickupTime ?pickupTime .\n",
    "    ?trip taxi:dropoffTime ?dropoffTime .\n",
    "    FILTER (?pickupTime >= \"2025-07-01T11:00:00\"^^xsd:dateTime &&\n",
    "            ?pickupTime <= \"2025-07-01T12:00:00\"^^xsd:dateTime)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Run the SPARQL query on the RDF graph and print results\n",
    "for row in g.query(query_hardcoded):\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SemanticModelling",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
